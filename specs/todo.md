- Objectif: un chat bot orienté code en mode CLI, utilisant l'IA avec un LLM local orienté code
- Technologies à utiliser: NodeJS, Genkit JS pour la partie AI, Docker Model Runner pour les LLMs locaux, Docker Agentic Compose pour l'exécution
  - les dépendances sont: {  "dependencies": {    "@genkit-ai/compat-oai": "^1.28.0",    "prompts": "^2.4.2"  },  "type": "module"}
  - le projet Genkit JS est disponible ici: https://github.com/firebase/genkit/tree/main/js
  - la documentation de Genkit JS est disponible ici: https://genkit.dev/docs/get-started/
  - la documentation de docker model runner est disponible ici: https://docs.docker.com/ai/model-runner/
  - Docker Model Runner par défaut écoute sur 12434
  - L'api de docker model runner est compatible OpenAI
  - On ne travaillera qu'avec des modèles générant du texte (avec du texte en entrée aussi)
  - Tu pourras trouver des exemples ici: https://k33g.org/GRP-Genkit%20JS-01-Baby%20steps.html
- LLM à utiliser: hf.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF:Q4_K_M
  - Il faudra charger le modèle avec la commande : docker model pull hf.co/Qwen/Qwen2.5-Coder-3B-Instruct-GGUF:Q4_K_M
- Fonctionnalités: 1ère partie
  - Afficher un prompt utilisateur (utiliser prompt.js) permettant à l'utilisateur de poser une question
  - Transmettre la question au LLM (via docker model runner)
  - La complétion/réponse du LLM se fera en mode streaming
  - Une fois la complétion terminée, revenir au prompt utilisateur
  - Permettre de quitter avec une commande /bye
  - Il faudra pouvoir paramétrer des instructions systèmes pour le chat bot dans un fichier markdown coding.instructions.md
    - Proposer un contenu pour ce fichier
- Fonctionnalités: 2ème partie
  - Gérer la mémoire conversationnelle
  - Permettre d'afficher la liste des messages de l'historique avec une commande /history
  - Permettre de vider l'historique système avec une commande /clear
    - le message initial de system instructions est conservé
- Fonctionnalités: 3ème partie
  - Il faut dockeriser l'application
    - Créer le Dockerfile pour l'application NodeJS
    - Créer le compose file associé en utilisant les spécificités associées de l'intégration de docker model runner dans agentic compose
      - La documentation de agentic compose est disponible ici: https://docs.docker.com/ai/compose/models-and-compose/
      - il faudra pouvoir paramétrer avec des variable d'environnement l'application:
        - nom du modèle (utiliser l'intégration docker model runner)
        - endpoint docker modèle runner (utiliser l'intégration docker model runner)
        - temperature
        - top_p
        - top_k
      - migrer le fichier markdown coding.instructions.md dans une config du compose file
        - documentation des configs ici: https://docs.docker.com/reference/compose-file/configs
        - utiliser `content` comme source de la config
    - on lancera le chat bot de cette manière:
      - docker compose up -d
      - docker compose exec simple-chat node index.js
